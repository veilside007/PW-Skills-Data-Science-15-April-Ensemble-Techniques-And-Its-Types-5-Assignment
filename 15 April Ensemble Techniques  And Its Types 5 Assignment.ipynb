{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4be65e-4bd6-4122-9d96-c5946cf588f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. You are working on a machine learning project where you have a dataset containing numerical and categorical features. You have identified that some of the features are highly correlated and there are missing values in some of the columns. You want to build a pipeline that automates the feature engineering process and handles the missing values.\n",
    "\n",
    "Design a pipeline that includes the following steps:\n",
    "\n",
    "• Use an automated feature selection method to identify the important features in the dataset.\n",
    "\n",
    "• Create a numerical pipeline that includes the following steps:\n",
    "\n",
    "• Impute the missing values in the numerical columns using the mean of the column values. Scale the numerical columns using standardisation.\n",
    "\n",
    "• Create a categorical pipeline that includes the following steps:\n",
    "\n",
    "• Impute the missing values in the categorical columns using the most frequent value of the column. • One-hot encode the categorical columns.\n",
    "\n",
    "• Combine the numerical and categorical pipelines using a Column Transformer. • Use a Random Forest Classifier to build the final model.\n",
    "\n",
    "• Evaluate the accuracy of the model on the test dataset.\n",
    "\n",
    "Note: Your solution should include code snippets for each step of the pipeline, and a brief explanation of each step. You should also provide an interpretation of the results and suggest possible improvements for the pipeline.\n",
    "                                                                                                                                                                                                                \n",
    "                                                                                                                                                                                                                \n",
    "Here's how you can design the pipeline to automate the feature engineering process and handle missing values:\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already defined\n",
    "\n",
    "# Step 1: Automated feature selection\n",
    "feature_selection = SelectFromModel(RandomForestClassifier())\n",
    "\n",
    "# Step 2: Numerical pipeline\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Step 3: Categorical pipeline\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Step 4: Combine numerical and categorical pipelines using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),  # assuming numerical_features is a list of numerical feature names\n",
    "        ('cat', categorical_pipeline, categorical_features)  # assuming categorical_features is a list of categorical feature names\n",
    "    ])\n",
    "\n",
    "# Step 5: Build the final model pipeline\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('feature_selection', feature_selection),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Step 6: Train the model\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Evaluate the accuracy of the model on the test dataset\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "Explanation of each step:\n",
    "1. Automated feature selection: Use a machine learning model (Random Forest Classifier in this case) to automatically select important features from the dataset.\n",
    "2. Numerical pipeline: Impute missing values in numerical columns using the mean of the column values and scale the numerical columns using standardization.\n",
    "3. Categorical pipeline: Impute missing values in categorical columns using the most frequent value of the column and one-hot encode the categorical columns.\n",
    "4. Combine numerical and categorical pipelines using ColumnTransformer: Apply the numerical pipeline to numerical features and the categorical pipeline to categorical features simultaneously.\n",
    "5. Build the final model pipeline: Combine the feature selection, preprocessing, and classification steps into a single pipeline.\n",
    "6. Train the model: Fit the pipeline on the training data to train the model.\n",
    "7. Evaluate the accuracy of the model: Predict the target variable on the test dataset and calculate the accuracy of the predictions.\n",
    "\n",
    "Interpretation of results:\n",
    "- The accuracy score obtained on the test dataset indicates how well the model performs in predicting the target variable.\n",
    "- Improvements can be made by fine-tuning hyperparameters of the RandomForestClassifier, exploring other feature selection methods, or trying different preprocessing techniques.\n",
    "\n",
    "Possible improvements for the pipeline:\n",
    "- Experiment with different machine learning models for feature selection and classification.\n",
    "- Try more advanced imputation techniques for handling missing values.\n",
    "- Explore different scaling methods for numerical features.\n",
    "- Consider incorporating additional preprocessing steps such as feature engineering or dimensionality reduction techniques.                                                                                                                                                                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a5d42c-dde1-48ce-b2bb-31bc9476a6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Build a pipeline that includes a random forest classifier and a logistic regression classifier, and then use a voting classifier to combine their predictions. Train the pipeline on the iris dataset and evaluate its accuracy.\n",
    "\n",
    "Here's how you can build a pipeline that includes a Random Forest Classifier and a Logistic Regression Classifier, and then use a Voting Classifier to combine their predictions on the Iris dataset:\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the individual classifiers\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "lr_classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Create a pipeline for each classifier\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('rf_classifier', rf_classifier)\n",
    "])\n",
    "\n",
    "lr_pipeline = Pipeline(steps=[\n",
    "    ('lr_classifier', lr_classifier)\n",
    "])\n",
    "\n",
    "# Create a voting classifier combining the individual classifiers\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('rf', rf_pipeline),\n",
    "    ('lr', lr_pipeline)\n",
    "], voting='soft')  # 'soft' voting for probabilities voting\n",
    "\n",
    "# Train the pipeline\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the accuracy of the pipeline on the test set\n",
    "accuracy = voting_classifier.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "This code:\n",
    "- Loads the Iris dataset and splits it into training and testing sets.\n",
    "- Defines individual classifiers: Random Forest Classifier (`rf_classifier`) and Logistic Regression Classifier (`lr_classifier`).\n",
    "- Creates pipelines for each classifier (`rf_pipeline` and `lr_pipeline`).\n",
    "- Creates a Voting Classifier (`voting_classifier`) combining the individual classifiers.\n",
    "- Trains the pipeline on the training data.\n",
    "- Evaluates the accuracy of the pipeline on the test set.\n",
    "\n",
    "The Voting Classifier combines the predictions from multiple classifiers and outputs the class with the highest majority of votes (soft voting in this case, which considers class probabilities). Finally, the accuracy of the Voting Classifier is printed, indicating how well the combined model performs on the test dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
